{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4decdacf-01d5-4ecf-a174-63676695b275",
   "metadata": {},
   "source": [
    "## 2022/09/05 Lecture \n",
    "- Lecturer: 周子軒學長\n",
    "- Content: Language Model, Perplexity, Pytorch RNN Architecture (Text Generation)\n",
    "- Scripts: \n",
    "    - https://github.com/ProFatXuanAll/language-model-playground\n",
    "    - `lmp/script/train_model.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f97e0307-18fa-464b-9006-33d9e456b7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trigram\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "n = 3 \n",
    "\n",
    "corpus = [\n",
    "    \"I like apple.\",\n",
    "    \"I like banana.\"\n",
    "]\n",
    "\n",
    "def preproc(s, n):\n",
    "    return '#'*n+s\n",
    "corpus = [preproc(x) for x in corpus]\n",
    "CondFreq = defaultdict(int)\n",
    "def to_ngram(sent, n = 3):\n",
    "    tokens = []\n",
    "    for i in range(len(sent)-n):\n",
    "        token = sent[i:i+n]\n",
    "        tokens.append(token)\n",
    "        # print(token[:-1])\n",
    "        CondFreq[token[:-1]] += 1 \n",
    "    return tokens\n",
    "corpus = [to_ngram(sent) for sent in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4d01fe49-fa8b-4046-a3c4-134b39faf828",
   "metadata": {},
   "outputs": [],
   "source": [
    "Freq = Counter([item for sublist in corpus for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "03675c69-b4ca-471a-b180-9198e19bc75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_freq(x):\n",
    "    if x not in Freq: return 0 \n",
    "    return Freq[x]/CondFreq[x[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "975a36a1-2bf9-4e3c-935b-9e63bfb245fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['##I', '#I ', 'I l', ' li', 'lik', 'ike', 'ke ', 'e a', ' ap', 'app', 'ppl', 'ple']\n",
      "the sentence's occurring prob 0.5\n",
      "bits per char: 1.0\n",
      "perpexity: 2.0\n",
      "['##I', '#I ', 'I l', ' li', 'lik', 'ike', 'ke ', 'e b', ' ba', 'ban', 'ana', 'nan', 'ana']\n",
      "the sentence's occurring prob 0.5\n",
      "bits per char: 1.0\n",
      "perpexity: 2.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "for sentence in corpus:\n",
    "    print(sentence)\n",
    "    prob = 1\n",
    "    for token in sentence:\n",
    "        prob *= conditional_freq(token)\n",
    "    bpc = - math.log2(prob)\n",
    "    ppl = math.pow(2, bpc)\n",
    "    print(f\"the sentence's occurring prob {prob}\") \n",
    "    print(f\"bits per char: {bpc}\")\n",
    "    print(f\"perpexity: {ppl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59b0e58-64f6-4813-b9e9-458676aa179f",
   "metadata": {},
   "source": [
    "\n",
    "### Intro\n",
    "   - https://github.com/ProFatXuanAll/language-model-playground\n",
    "- Byte-Pair Encoding\n",
    "- Character: \n",
    "    - 'I': 1 byte, \n",
    "    - '我':~=3 bytes (variable-length)\n",
    "- Splitting:\n",
    "    - Tokenization: splitting a sentence, may not be sensible (in English we can use space to do so)\n",
    "    -  (Word) Segmentation: Sensible Tokenization (e.g. in Chinese)\n",
    "- Affix/Prefix/Suffix/(Infix)\n",
    "- https://github.com/ProFatXuanAll/language-model-playground 中只有提供tokenization，沒有WS。\n",
    "\n",
    "- long-tail phenomenon (Zipf's Law): min_count 砍尾巴、或是使用max_vocab\n",
    "- normalize (lower/upper-case, )\n",
    "\n",
    "### Train \n",
    "- Approximate the generation prob to a distribution\n",
    "- $ P(x;\\theta)$, $\\theta$ is the model (e.g. if use Gaussian, $\\theta$ is the mean and std)\n",
    "    - Hyperparameters: 我們調整的參數\n",
    "    - Parameters: 模型使用訓練更新的參數\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "882fdb7e-1b17-453e-b3f2-b66c1bb8822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "classes = {1:2, 2:4, 3:6}\n",
    "# normalization\n",
    "norm = {k:v/sum(classes.values()) for k, v in classes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d3d69ea1-9c57-4c4a-a827-85e4e0f3c466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.16666666666666666, 2: 0.3333333333333333, 3: 0.5}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c17cef83-c2dd-4491-a8e1-2528d01fda60",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = defaultdict(int)\n",
    "for k in classes:\n",
    "    cdf[k] = norm[k]\n",
    "    cdf[k] += cdf[k-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fc999f2b-587f-4df8-b23f-56b43be1de23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {1: 0.16666666666666666, 0: 0, 2: 0.5, 3: 1.0})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "07f3e7af-26c3-475f-b663-73ce95d057aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1.0: 5098, 0.5: 3224, 0.16666666666666666: 1678})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_sample = 10000\n",
    "results = []\n",
    "\n",
    "for i in range(n_sample):\n",
    "    rn = random.uniform(0,1)\n",
    "    for idx, _cdf in enumerate(sorted(cdf.values())):\n",
    "        if rn < _cdf:\n",
    "            results.append(cdf[idx])\n",
    "            break\n",
    "c = Counter(results)\n",
    "print(c)\n",
    "# inverted probability density function \n",
    "# 抽樣看每個區間的大小"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e416a823-edb9-4bb6-9592-3a17a229707d",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "用`register_buffer()`寫在model底下，會讓Pytorch記得把這個tensor搬到正確的device上。\n",
    "```\n",
    "self.register_buffer(name=..., tensor=torch.tensor(...))\n",
    "```\n",
    "slicing tensor 非常慢，因為要確認記憶體連續與否等，最好以整份tensor做運算。\n",
    "在LSTM, RNN這種recurrence seq models裡面，hidden states的計算是count on前一個hidden states，因此有一段for loop是絕對無法平行化的，這就是這種神經網路為何seq越長越慢，且無法加速。\n",
    "Transformers則可以跳脫這個constraint。\n",
    "\n",
    "text-generation的概念和classification是一樣的 e.g. calc similarity scores \n",
    "只是output前再跟embedding matrix互動一次，然後predict category # = vocab_size。\n",
    "\n",
    "去看`lmp/script/train_model.py` 看那個get_optimizer怎麼寫！！\n",
    "學長說只能if else或是dictionary lookupㄌ...哭ㄌ...。\n",
    "\n",
    "weight decay:\n",
    "$$y = Wx+b$$\n",
    "$$y + \\Delta y = W(x+\\Delta x) + b$$\n",
    "我想要 $\\Delta x$ 和 $\\Delta y$ 的大小變動可以一致，因此要控制W內的參數大小範圍。\n",
    "但不要對b(bias vector)做weight decay！！！\n",
    "請去看 `lmp/script/train_model.py`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcd2d1a-f799-48c7-ae93-fe460ddf5255",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
